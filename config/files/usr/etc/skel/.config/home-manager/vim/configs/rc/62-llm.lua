-- local llm = require('llm')
-- llm.setup({
--   model = "bigcode/starcoder", -- can be a model ID or an http(s) endpoint
--   tokens_to_clear = { "<|endoftext|>" }, -- tokens to remove from the model's output
--   -- parameters that are added to the request body
--   query_params = {
--     max_new_tokens = 60,
--     temperature = 0.2,
--     top_p = 0.95,
--     stop_tokens = nil,
--   },
--   -- set this if the model supports fill in the middle
--   fim = {
--     enabled = true,
--     prefix = "<fim_prefix>",
--     middle = "<fim_middle>",
--     suffix = "<fim_suffix>",
--   },
--   debounce_ms = 150,
--   accept_keymap = "<S-CR>",
--   -- dismiss_keymap = "<S-Tab>",
--   tls_skip_verify_insecure = false,
--   tokenizer = nil, -- cf Tokenizer paragraph
--   context_window = 8192, -- max number of tokens for the context window
--   enable_suggestions_on_startup = false,

-- })

-- local completion = require("llm.completion")
-- function _G.get_llm_suggestion()
--   if not completion.suggestion then
--     completion.lsp_suggest()
--   else
--     completion.complete()
--   end
-- end
-- -- pcall to consume some odd error during the completion() call.
-- vim.api.nvim_set_keymap("n", "<S-CR>", "<cmd>lua pcall(get_llm_suggestion)<CR>", { noremap = true })
-- vim.api.nvim_set_keymap("i", "<S-CR>", "<C-o>:lua pcall(get_llm_suggestion)<CR>", { noremap = true })
